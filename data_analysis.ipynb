{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats as stats"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_button_a = pd.read_csv(\"table_button_a.csv\")\n",
    "table_time_a = pd.read_csv(\"table_time_a.csv\")\n",
    "table_button_b = pd.read_csv(\"table_button_b.csv\")\n",
    "table_time_b = pd.read_csv(\"table_time_b.csv\")\n",
    "\n",
    "qualtrics_data = pd.read_csv(\"OpenGÃ¡ta_Website_testing_Students_6_June_2023_final.csv\")\n",
    "# Dropping the first 2 rows from the dataset that are not responses\n",
    "qualtrics_data = qualtrics_data.drop([0, 1])\n",
    "# Only use completed responses\n",
    "qualtrics_data = qualtrics_data[qualtrics_data[\"Finished\"] == \"True\"]\n",
    "# Only using \"real\" responses\n",
    "qualtrics_data = qualtrics_data[qualtrics_data[\"Status\"] != \"Spam\"]\n",
    "# Converts the Random ID variable to a float\n",
    "qualtrics_data[\"Random ID\"] = qualtrics_data[\"Random ID\"].astype(float)\n",
    "\n",
    "df_button_a = pd.DataFrame(table_button_a).dropna(subset=\"visitor_id\")\n",
    "df_time_a = pd.DataFrame(table_time_a).dropna(subset=\"visitor_id\")\n",
    "df_button_b = pd.DataFrame(table_button_b).dropna(subset=\"visitor_id\")\n",
    "df_time_b = pd.DataFrame(table_time_b).dropna(subset=\"visitor_id\")\n",
    "\n",
    "# Uses only the user-tracking data if the visitor ID is also in the list of Qualtrics random IDs \n",
    "df_button_a = df_button_a.loc[df_button_a.visitor_id.isin(qualtrics_data[\"Random ID\"])]\n",
    "df_time_a = df_time_a.loc[df_time_a.visitor_id.isin(qualtrics_data[\"Random ID\"])]\n",
    "df_button_b = df_button_b.loc[df_button_b.visitor_id.isin(qualtrics_data[\"Random ID\"])]\n",
    "df_time_b = df_time_b.loc[df_time_b.visitor_id.isin(qualtrics_data[\"Random ID\"])]\n",
    "\n",
    "display(df_button_a)\n",
    "display(df_time_a)\n",
    "display(df_button_b)\n",
    "display(df_time_b)\n",
    "display(qualtrics_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Is time data normal?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_data_normal(dataframe):\n",
    "    _, p_value = stats.shapiro(dataframe)\n",
    "    \n",
    "    significance_level = 0.05  # Adjust the significance level as needed\n",
    "    \n",
    "    if p_value > significance_level:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def normal_values(dataframe, column, data_type, version):\n",
    "    # Retrieve the column data from the dataframe\n",
    "    data = dataframe[column]\n",
    "\n",
    "    # Check if the column data is normally distributed\n",
    "    is_normal = is_data_normal(data)\n",
    "    print(f\"Is the {data_type} data for version {version} normally distributed?\", is_normal)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_values(df_time_a, \"time_spent\", \"time\", \"A\")\n",
    "normal_values(df_time_b, \"time_spent\", \"time\", \"B\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Excluding outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outliers(df, column, normal):\n",
    "    column = str(column)\n",
    "\n",
    "    if normal == True:\n",
    "        std = np.std(df[column])\n",
    "        mean = np.mean(df[column])\n",
    "\n",
    "        upper_lim = mean + 3*std\n",
    "        lower_lim = mean - 3*std\n",
    "\n",
    "        df_no_outliers = df[(df[column] < upper_lim) & (df[column] > lower_lim)]\n",
    "        return df_no_outliers\n",
    "    \n",
    "    elif normal == False:\n",
    "        df_sorted = df.sort_values(by=[column], ascending=True)\n",
    "\n",
    "        q1 = df_sorted[column].quantile(0.25)\n",
    "        q3 = df_sorted[column].quantile(0.75)\n",
    "        iqr = q3-q1\n",
    "\n",
    "        upper_lim = q3 + 1.5*iqr\n",
    "        lower_lim = q1 - 1.5*iqr\n",
    "\n",
    "        print(f\"q1 = {q1}, q3 = {q3}, iqr = {iqr}, upper bound = {upper_lim}, lower bound = {lower_lim}\")\n",
    "\n",
    "        df_sorted = df_sorted.sort_values(by=[\"id\"], ascending=True)\n",
    "\n",
    "        df_no_outliers = df_sorted[(df_sorted[column] < upper_lim) & (df_sorted[column] > lower_lim)]\n",
    "        return df_no_outliers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time spent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time_a = outliers(df=df_time_a, column=\"time_spent\", normal=False)\n",
    "df_time_a.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time_b = outliers(df=df_time_b, column=\"time_spent\", normal=False)\n",
    "df_time_b.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reshaping data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transforming from long to wide, adds \"Total\" row and column\n",
    "df_button_wide_a = pd.pivot_table(data=df_button_a, index = \"visitor_id\", columns = \"name\", values = \"button\", aggfunc=\"sum\", \\\n",
    "                                  margins=True, margins_name=\"Total\")\n",
    "df_time_wide_a = pd.pivot_table(df_time_a, index = \"visitor_id\", columns = \"page\", values=\"time_spent\", aggfunc=\"sum\", \\\n",
    "                                margins=True, margins_name=\"Total\")\n",
    "df_button_wide_b = pd.pivot_table(df_button_b, index = \"visitor_id\", columns = \"name\", values = \"button\", aggfunc=\"sum\", \\\n",
    "                                  margins=True, margins_name=\"Total\")\n",
    "df_time_wide_b = pd.pivot_table(df_time_b, index = \"visitor_id\", columns = \"page\", values=\"time_spent\", aggfunc=\"sum\", \\\n",
    "                                margins=True, margins_name=\"Total\")\n",
    "\n",
    "# Removing the \"Total\" row\n",
    "df_button_wide_a = df_button_wide_a.drop(df_button_wide_a.index[-1])\n",
    "df_time_wide_a = df_time_wide_a.drop(df_time_wide_a.index[-1])\n",
    "df_button_wide_b = df_button_wide_b.drop(df_button_wide_b.index[-1])\n",
    "df_time_wide_b = df_time_wide_b.drop(df_time_wide_b.index[-1])\n",
    "\n",
    "df_button_wide_a = df_button_wide_a.reset_index()\n",
    "df_time_wide_a = df_time_wide_a.reset_index()\n",
    "df_button_wide_b = df_button_wide_b.reset_index()\n",
    "df_time_wide_b = df_time_wide_b.reset_index()\n",
    "\n",
    "display(df_button_wide_a)\n",
    "display(df_time_wide_a)\n",
    "display(df_button_wide_b)\n",
    "display(df_time_wide_b)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qualtrics data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning qualtrics data\n",
    "qualtrics_data = qualtrics_data.drop(qualtrics_data.columns[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]], axis=1)\n",
    "\n",
    "# Replacing Likert-scale answers with numeric values\n",
    "qualtrics_data = qualtrics_data.replace(\"7 - Strongly agree\", 7)\n",
    "qualtrics_data = qualtrics_data.replace(\"6 - Agree\", 6)\n",
    "qualtrics_data = qualtrics_data.replace(\"5 - Somewhat agree\", 5)\n",
    "qualtrics_data = qualtrics_data.replace(\"4 - Neither agree nor disagree\", 4)\n",
    "qualtrics_data = qualtrics_data.replace(\"3 - Somewhat disagree\", 3)\n",
    "qualtrics_data = qualtrics_data.replace(\"2 - Disagree\", 2)\n",
    "qualtrics_data = qualtrics_data.replace(\"1 - Strongly disagree\", 1)\n",
    "\n",
    "# Seperating the data of A/B versions & inverting questions worded differently on the Likert Scale\n",
    "df_qualtrics_a = qualtrics_data[qualtrics_data['version'] == 'A']\n",
    "df_qualtrics_b = qualtrics_data[qualtrics_data['version'] == 'B']\n",
    "\n",
    "df_qualtrics_a.dropna()\n",
    "df_qualtrics_b.dropna()\n",
    "\n",
    "df_qualtrics_a.reset_index()\n",
    "df_qualtrics_b.reset_index()\n",
    "\n",
    "# Easy to use questions\n",
    "df_a_easy = df_qualtrics_a.iloc[:,[1, 6, 7, 10]]\n",
    "df_b_easy = df_qualtrics_b.iloc[:,[1, 6, 7, 10,]]\n",
    "\n",
    "df_a_easy.iloc[:, 2] = 8 - df_a_easy.iloc[:, 2]\n",
    "df_b_easy.iloc[:, 2] = 8 - df_b_easy.iloc[:, 2]\n",
    "\n",
    "# Trust questions\n",
    "df_a_trust = df_qualtrics_a.iloc[:,[3, 4, 5, 10]]\n",
    "df_b_trust = df_qualtrics_b.iloc[:,[3, 4, 5, 10]]\n",
    "\n",
    "df_a_trust.iloc[:, 1:3] = 8 - df_a_trust.iloc[:, 1:3]\n",
    "df_b_trust.iloc[:, 1:3] = 8 - df_b_trust.iloc[:, 1:3]\n",
    "\n",
    "# Informative website questions\n",
    "df_a_info = df_qualtrics_a.iloc[:,[2, 8, 9, 10]]\n",
    "df_b_info = df_qualtrics_b.iloc[:,[2, 8, 9, 10]]\n",
    "\n",
    "df_a_info.iloc[:, [0, 2]] = 8 - df_a_info.iloc[:, [0, 2]]\n",
    "df_b_info.iloc[:, [0, 2]] = 8 - df_b_info.iloc[:, [0, 2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_a_easy)\n",
    "display(df_b_easy)\n",
    "\n",
    "display(df_a_trust)\n",
    "display(df_b_trust)\n",
    "\n",
    "display(df_a_info)\n",
    "display(df_b_info)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding total scores"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qualtrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addtotal(dataframe):\n",
    "    dataframe[\"Total\"] = dataframe.drop(\"Q3.1\", axis=1).sum(axis=\"columns\")\n",
    "    return dataframe\n",
    "\n",
    "df_a_easy = addtotal(df_a_easy)\n",
    "df_b_easy = addtotal(df_b_easy)\n",
    "\n",
    "df_a_trust = addtotal(df_a_trust)\n",
    "df_b_trust = addtotal(df_b_trust)\n",
    "\n",
    "df_a_info = addtotal(df_a_info)\n",
    "df_b_info = addtotal(df_b_info)\n",
    "\n",
    "df_a_easy = df_a_easy.reset_index()\n",
    "df_b_easy = df_b_easy.reset_index()\n",
    "\n",
    "df_a_trust = df_a_trust.reset_index()\n",
    "df_b_trust = df_b_trust.reset_index()\n",
    "\n",
    "df_a_info = df_a_info.reset_index()\n",
    "df_b_info = df_b_info.reset_index()\n",
    "\n",
    "display(df_a_easy)\n",
    "display(df_b_easy)\n",
    "display(df_a_trust)\n",
    "display(df_b_trust)\n",
    "display(df_a_info)\n",
    "display(df_b_info)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Is button data normal?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_values(df_button_wide_a, \"Total\", \"button click\", \"A\")\n",
    "normal_values(df_button_wide_b, \"Total\", \"button click\", \"B\")\n",
    "\n",
    "normal_values(df_a_trust, \"Total\", \"trust\", \"A\")\n",
    "normal_values(df_b_trust, \"Total\", \"trust\", \"B\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mann-Whitney U test / Wilcoxon Rank-sum test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conduct_test(group1, group2, alternative):\n",
    "    _, p_value = stats.ranksums(group1, group2, alternative=alternative)\n",
    "    \n",
    "    significance_level = 0.05\n",
    "    \n",
    "    if p_value < significance_level:\n",
    "        print(f\"The p-value is {p_value}, and the significance level is {significance_level}\")\n",
    "        print(\"There is a significant difference between the groups.\")\n",
    "    else:\n",
    "        print(f\"The p-value is {p_value}, and the significance level is {significance_level}\")\n",
    "        print(\"There is no significant difference between the groups.\")\n",
    "\n",
    "def wrs_test(df_a, df_b, column):\n",
    "    # Retrieve the column data from the dataframe\n",
    "    a_data = df_a[column]\n",
    "    b_data = df_b[column]\n",
    "\n",
    "    # Perform the Wilcoxon rank-sum test\n",
    "    conduct_test(a_data, b_data, alternative='greater')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time spent - not normally distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrs_test(df_time_wide_a, df_time_wide_b, \"Total\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def welch_t_test(trust_a, trust_b):\n",
    "    _, p_value = stats.ttest_ind(trust_a, trust_b, equal_var=False)\n",
    "    \n",
    "    significance_level = 0.05\n",
    "    \n",
    "    if p_value < significance_level:\n",
    "        print(f\"The p-value is {p_value}, and the significance level is {significance_level}\")\n",
    "        print(\"There is a significant difference between the groups.\")\n",
    "    else:\n",
    "        print(f\"The p-value is {p_value}, and the significance level is {significance_level}\")\n",
    "        print(\"There is no significant difference between the groups.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trust - normally distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "welch_t_test(df_a_trust[\"Total\"], df_b_trust[\"Total\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For buttons clicked - normally distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "welch_t_test(df_button_wide_a[\"Total\"], df_button_wide_b[\"Total\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
